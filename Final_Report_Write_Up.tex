\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Final Report for Red Wine Analysis},
            pdfauthor={Ruiqiang Chen, Michael DeWitt, David Williams, Alex Vannoy},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{Final Report for Red Wine Analysis}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{Ruiqiang Chen, Michael DeWitt, David Williams, Alex Vannoy}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{7/28/2017}

\usepackage{amsmath}
\usepackage{float}

\begin{document}
\maketitle

\section{Introduction}\label{introduction}

The purpose of this document is to report the proposed statistical
models for classification of red wine bases on eleven predictors. The
purpose of this analysis is to provide a model to the vintners in order
for them to better predict the quality rating for their product.
Analysis will be performing using both regression techniques and
classification techniques.

\section{Description of Data}\label{description-of-data}

The data set provided is the Wine dataset from UC Irvine of red
\emph{vinho verde} wine samples, from the north of Portugal {[}Cortez et
al., 2009{]}. It consists of 1599 with a total of 11 physicochemical
predictors and a response variable. These predictors include the
following: fixed acidity, volatile acidity, citric acid, residual sugar,
chlorides, free sulfur dioxide, total sulfur dioxide, density, pH,
sulphates, and alcohol with the quality feature being associated with
the judgement of the individual wine's quality. Quality is the feature
of interest for the dataset as the vintner is interested in judging the
wine's quality through objective means rather than today's subjective
method of averaging the 1one to ten point judgment of taste-testers. A
summary of these measures as well as the response variable can be seen
in Table 1.

\begin{longtable}[]{@{}lrrrr@{}}
\caption{Summary Statistics for the Wine Dataset}\tabularnewline
\toprule
Descriptions & min & median & mean & max\tabularnewline
\midrule
\endfirsthead
\toprule
Descriptions & min & median & mean & max\tabularnewline
\midrule
\endhead
fixed acidity (g(tartaric acid)/dm\^{}3) & 4.60 & 7.90 & 8.32 &
15.90\tabularnewline
volatile acidity (g(acetic acid)/dm\^{}3) & 0.12 & 0.52 & 0.53 &
1.58\tabularnewline
citric acid (g/dm\^{}3) & 0.00 & 0.26 & 0.27 & 1.00\tabularnewline
residual sugar (g/dm\^{}3) & 0.90 & 2.20 & 2.54 & 15.50\tabularnewline
chlorides (g(sodium cloride)/dm\^{}3 & 0.01 & 0.08 & 0.09 &
0.61\tabularnewline
free sulfur dioxide (mg/dm\^{}3) & 1.00 & 14.00 & 15.87 &
72.00\tabularnewline
total sulfur dioxide (mg/dm\^{}3) & 6.00 & 38.00 & 46.47 &
289.00\tabularnewline
density (g/cm\^{}3) & 0.99 & 1.00 & 1.00 & 1.00\tabularnewline
pH & 2.74 & 3.31 & 3.31 & 4.01\tabularnewline
sulphates (g(potassium sulphate)/dm\^{}3) & 0.33 & 0.62 & 0.66 &
2.00\tabularnewline
alcohol (\% vol.) & 8.40 & 10.20 & 10.42 & 14.90\tabularnewline
quality & 3.00 & 6.00 & 5.64 & 8.00\tabularnewline
\bottomrule
\end{longtable}

The distribution of these different criteria can be seen below in the
histograms in Figure 1.

\begin{figure}[H]
\centering
\includegraphics{Final_Report_Write_Up_files/figure-latex/global_histograms-1.pdf}
\caption{Histogram of all variables in the data set}
\end{figure}

The following are slightly right skewed: Fixed Acidity, Volatile
Acidity, Citric Acid, , Free Sulfur Dioxide, Total Sulfur Dioxide,
Sulphates, and Alcohol. Residual Sugar and Chlorides are heavily right
skewed with density and pH appearing more normally distributed.
Completing a Shapiro Wilke normality test on the components indicates
that all are non-normal. Reviewing the individual components there
appears to be a slight irregularity with total free sulfur dioxide. This
can be seen in the histogram of free sulfur dioxide values.

\begin{figure}[H]
\centering
\includegraphics{Final_Report_Write_Up_files/figure-latex/so2_fig-1.pdf}
\caption{Histogram of Sulfur Dioxide Predictor}
\end{figure}

As well as the fit of free sulfur dioxide display high studentized
residuals and leverage and thus should be considered for removal in the
modeling process. These wines are 1080 and 1082.

\begin{figure}[htbp]
\centering
\includegraphics{Final_Report_Write_Up_files/figure-latex/so2_resid-1.pdf}
\caption{Residual Plots of Linear Model Predicting Quality with Total
Sulfur Dioxide}
\end{figure}

These two wines have been removed from the clean dataset in order to be
better predictors. The presence of these two wines may result in
incorrect or inaccurate predictions. Ass we did not gather this dataset,
we do not know if this information was incorrectly captured or if these
values are real. However, given the strong indication that these two
points are outliers with high leverage it is a good assumption to remove
these two points.

\section{Method}\label{method}

In order to estimate the test error of any of the generated models, the
data was divided in testing and training data sets with which to train
then models and then test and estimate the testing error. Seventy
percent of the raw data was randomly selected and placed in the training
set with the remaining thirty percent used as the testing data set.

\subsection{Regression}\label{regression}

To select the best fit regression model, several regression modeling
methods were used: Least Squares Regression, Ridge Regression, Lasso
Regression, Principle Components Regression, Partial Least Squares
Regression, and Boosting. In each regression the dependent variable was
the integer value of Quality. The data were divided into two sets, a
training set to train the model and a testing set for model validation.
We will now go deeper in the model generation process for each of these
modeling types and methods

\subsubsection{Least Squares}\label{least-squares}

The least squares regression method that was tested was the best subset
selection. The methodology used to determine the best subset model was
to first run cross-validation on the training set in order to determine
the best number of predictors to include in the model. This analysis
indicated that any additional predictor after three variables did not
increase the accuracy of the model. The training data were then used to
determine the best subset of the linear model with three predictors. The
best subset included volatile acidity, sulphates, and alcohol.

\paragraph{Residual Analysis}\label{residual-analysis}

The linear regression model requires the assumption that residuals from
the model have constant, unrelated variances. The plots below show the
fitted values versus the residual values and illustrate that we satisfy
the assumptions required to conduct a linear regression.

\begin{figure}[htbp]
\centering
\includegraphics{Final_Report_Write_Up_files/figure-latex/residuals-1.pdf}
\caption{Plot of Residuals from Linear Regression}
\end{figure}

The residuals appear to have no distinct pattern which is a positive
sign that there are not lurking relationships that have not been treated
by the modeling.

\subsubsection{Ridge Regression}\label{ridge-regression}

Ridge regression was performed on the dataset as well. Cross-validation
was performed on the training data set to determine the optimum value
for lambda for the ridge regression. This lambda, 0.079 was then used in
a ridge regression model with the testing dataset. Further, there seems
to be a very large coefficient with 11 much smaller coefficients.

\subsubsection{Lasso Regression}\label{lasso-regression}

Next Lasso regression was performed with cross-validation. We conducted
cross-validation to determine the optimal value for lambda for the Lasso
regression. This value was 0.005. As a function of the Lasso regression,
only pH was shrunk to zero; total sulfur dioxide and free sulfur dioxide
were shrunk to near zero. As was the case with ridge regression, there
appears to be one large coefficient compared to the others.

\begin{figure}[H]

{\centering \includegraphics[width=225px]{Final_Report_Write_Up_files/figure-latex/lasso-1} \includegraphics[width=225px]{Final_Report_Write_Up_files/figure-latex/lasso-2} 

}

\caption{Plot of Lambda vs Coefficients for Ridge/Lasso Regression}\label{fig:lasso}
\end{figure}

\subsubsection{Principal Components
Regression}\label{principal-components-regression}

Next we performed Principal components regression. Analysis of the
principal components revealed that 90 percent of the variation in the
response could be explained by the first nine components, so we used
these first nine principal components for model training. This is
graphically displayed in the plot of principal components.

\begin{figure}[htbp]
\centering
\includegraphics{Final_Report_Write_Up_files/figure-latex/pcr_fig-1.pdf}
\caption{Variance Explained for Principal Components Regression}
\end{figure}

\subsubsection{Partial Least Squares
Regression}\label{partial-least-squares-regression}

Next we performed partial least squares regression, using the response
variable Quality as supervision over the principal components. Using
this method, we see from the plot of partial least squares components
that after roughly 2-3 components, model accuracy no longer
substantially increases.

\begin{figure}[htbp]
\centering
\includegraphics{Final_Report_Write_Up_files/figure-latex/pls_fig-1.pdf}
\caption{Variance Explained for Partial Least Squares Regression}
\end{figure}

\subsubsection{Boosted Regression}\label{boosted-regression}

inally we performed Boosted regression. The interaction depth was
limited to four in order to reduce the likelihood of over-fitting the
data. The model was trained on 5,000 different trees and we assumed a
Guassian distribution.

\begin{figure}[H]
\centering
\includegraphics{Final_Report_Write_Up_files/figure-latex/boost_importance-1.pdf}
\caption{Relative Importance from Boosted Regression}
\end{figure}

\subsubsection{Model Selection}\label{model-selection}

The resulting mean squared errors for each regression method were
tabulated in order to determine the superior model.

\begin{figure}[H]
\centering
\includegraphics{Final_Report_Write_Up_files/figure-latex/mse_results-1.pdf}
\caption{Plot of Results of Different Regression Techniques}
\end{figure}

\subsection{Classification}\label{classification}

For classification purposes the wines were segregated in to three
different classes. These classes include ``good'' (\(quality >7\)),
``medium'' (\(\; quality \; between \; 4 \;and\; 7\)) and
``poor''(\(quality < 4\)).

\subsubsection{Model Selection}\label{model-selection-1}

We conducted our analysis for this new variable of quality classes using
several classification models: K-Nearest Neighbors, Linear Discriminant
Analysis, Quadratic Discriminant Analysis, and tree classification. We
trained each model using the training data set and then applied the
resulting model to the testing dataset to estimate the model's test
error. It is important to note that we achieved greater accuracy by
scaling values for the K-Nearest Neighbors approach, which is sensitive
to scale differences across variables. Using unscaled values, the
validation algorithm was best using 17 variables; using scaled values,
64 were best. The larger number of neighbours makes for a much more
global model: it is less sensitive to immediate neighbours in the
bias-variance trade-off. The tree classification model was trained first
through cross-validation and then pruned to six leaves to reduce the
impact of over-fitting in the bias-variance trade-off.

\subsection{Comparison of Models}\label{comparison-of-models}

\begin{figure}[H]
\centering
\includegraphics{Final_Report_Write_Up_files/figure-latex/classification_results-1.pdf}
\caption{Plot of Results of Different Classification Techniques}
\end{figure}

All of these models seek to maximize the global accuracy of the model.
More interesting for the vintners is the ability to detect each of the
three different classes of the wines.

\begin{longtable}[]{@{}lrrr@{}}
\caption{Detailed Classification Accuracy}\tabularnewline
\toprule
Method & Good & Medium & Poor\tabularnewline
\midrule
\endfirsthead
\toprule
Method & Good & Medium & Poor\tabularnewline
\midrule
\endhead
KNN & 0.24 & 0.98 & 0.00\tabularnewline
Naive Bayes & 0.52 & 0.77 & 0.06\tabularnewline
LDA & 0.43 & 0.93 & 0.06\tabularnewline
QDA & 0.61 & 0.90 & 0.89\tabularnewline
Tree & 0.36 & 0.92 & 0.00\tabularnewline
\bottomrule
\end{longtable}

\section{Discussion}\label{discussion}

This analysis shows that for regression the boosted model resulted in
the highest accuracy of all regression models; however, this accuracy
comes at a cost of interpretability. Because the boosted algorithms have
little intrepretation this accuracy is more beneficial for prediction
than inference. If inference is the goal for the vintner and
horticulturalists who seek to understand the properties that make good
wines, the model with higher interpretability and the second highest
accuracy is the Lasso regression model. While the PLS is more accurate,
again it suffers from ease of interpretation.

Thus with this in mind, the best model for inference with high accuracy
is characterized by the below equation:

\begin{equation}
\begin{aligned}
\label{lasso_eq}
quality = 39.37 + 0.0823 * fixed\;acidity -0.981 * volatile\;acidity -0.405 * citric\;acid \\
-0.013 *residual\;sugar -1.075 * chlorides + 0.006 * free\;sulfur\;dioxide\\
- 0.002 * total\;sulfur\;dioxide - 37.09 * density +1.032 * sulphates + 0.256 * alcohol
\end{aligned}
\end{equation}

Whereas the best model for prediction is the boosted regression model.
An example tree is show below:

\begin{longtable}[]{@{}lrrrrrrrr@{}}
\caption{Example Boosted Regression Tree}\tabularnewline
\toprule
& SplitVar & SplitCodePred & LeftNode & RightNode & MissingNode &
ErrorReduction & Weight & Prediction\tabularnewline
\midrule
\endfirsthead
\toprule
& SplitVar & SplitCodePred & LeftNode & RightNode & MissingNode &
ErrorReduction & Weight & Prediction\tabularnewline
\midrule
\endhead
0 & 10 & 10.4500000 & 1 & 5 & 12 & 32.927684 & 240 &
-0.0000021\tabularnewline
1 & 9 & 0.6350000 & 2 & 3 & 4 & 9.971645 & 141 &
-0.0003125\tabularnewline
2 & -1 & -0.0005283 & -1 & -1 & -1 & 0.000000 & 85 &
-0.0005283\tabularnewline
3 & -1 & 0.0000152 & -1 & -1 & -1 & 0.000000 & 56 &
0.0000152\tabularnewline
4 & -1 & -0.0003125 & -1 & -1 & -1 & 0.000000 & 141 &
-0.0003125\tabularnewline
5 & 7 & 0.9952450 & 6 & 7 & 11 & 11.203147 & 99 &
0.0004400\tabularnewline
6 & -1 & 0.0008572 & -1 & -1 & -1 & 0.000000 & 39 &
0.0008572\tabularnewline
7 & 5 & 5.5000000 & 8 & 9 & 10 & 10.837500 & 60 &
0.0001687\tabularnewline
8 & -1 & -0.0006813 & -1 & -1 & -1 & 0.000000 & 12 &
-0.0006813\tabularnewline
9 & -1 & 0.0003812 & -1 & -1 & -1 & 0.000000 & 48 &
0.0003812\tabularnewline
10 & -1 & 0.0001687 & -1 & -1 & -1 & 0.000000 & 60 &
0.0001687\tabularnewline
11 & -1 & 0.0004400 & -1 & -1 & -1 & 0.000000 & 99 &
0.0004400\tabularnewline
12 & -1 & -0.0000021 & -1 & -1 & -1 & 0.000000 & 240 &
-0.0000021\tabularnewline
\bottomrule
\end{longtable}

Applying the equation \ref{lasso_eq} the vintner can examine each of the
variables independently and provide some degree of inference regarding
the chemical levels that influence the quality of the red wine. For
instance we see that sulfate content appears to have a strong positive
influence on wine quality while having additional residual sugars
reduces wine quality (as measured by the subjective wine quality score).
In the hands of the vintner, these relationships can be explored or
potentially exploited to produce a more consistent, higher quality wine.
\linebreak

Turning to the classification method, the best overall classification
method was Quadratic Discriminate Analysis. This is seen in both the
overall accuracy as well in its ability to accurately classify each
subcategory. While the other methods have lesser abilities to detect the
good and poor quality wines, the QDA method showed the best accuracy in
these two fields, which is very important for vintners when it comes to
pricing and selling. The penalty of misclassifying a good wine as medium
or a poor wine as medium/ good is severe as this may damage the
reputation of the winery. From this analysis it is clear that QDA is the
superior method for classification of red wines given this dataset. The
priors and group means for the QDA are shown in the preceding figures.

\begin{longtable}[]{@{}lr@{}}
\caption{Priors used in Quadratic Discriminate Analysis}\tabularnewline
\toprule
good & 0.140\tabularnewline
medium & 0.823\tabularnewline
poor & 0.038\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}ccccc@{}}
\caption{Group Means used in Quadratic Discriminate Analysis (continued
below)}\tabularnewline
\toprule
\begin{minipage}[b]{0.14\columnwidth}\centering\strut
~\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\centering\strut
fixed\_acidity\strut
\end{minipage} & \begin{minipage}[b]{0.21\columnwidth}\centering\strut
volatile\_acidity\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\centering\strut
citric\_acid\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\centering\strut
residual\_sugar\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.14\columnwidth}\centering\strut
~\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\centering\strut
fixed\_acidity\strut
\end{minipage} & \begin{minipage}[b]{0.21\columnwidth}\centering\strut
volatile\_acidity\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\centering\strut
citric\_acid\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\centering\strut
residual\_sugar\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.14\columnwidth}\centering\strut
\textbf{good}\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering\strut
8.56\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\centering\strut
0.409\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\centering\strut
0.325\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering\strut
2.592\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.14\columnwidth}\centering\strut
\textbf{medium}\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering\strut
8.306\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\centering\strut
0.527\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\centering\strut
0.263\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering\strut
2.487\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.14\columnwidth}\centering\strut
\textbf{poor}\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering\strut
7.694\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\centering\strut
0.68\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\centering\strut
0.129\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering\strut
3.061\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}ccccc@{}}
\caption{Table continues below}\tabularnewline
\toprule
\begin{minipage}[b]{0.14\columnwidth}\centering\strut
~\strut
\end{minipage} & \begin{minipage}[b]{0.13\columnwidth}\centering\strut
chlorides\strut
\end{minipage} & \begin{minipage}[b]{0.24\columnwidth}\centering\strut
free\_sulfur\_dioxide\strut
\end{minipage} & \begin{minipage}[b]{0.25\columnwidth}\centering\strut
total\_sulfur\_dioxide\strut
\end{minipage} & \begin{minipage}[b]{0.10\columnwidth}\centering\strut
density\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.14\columnwidth}\centering\strut
~\strut
\end{minipage} & \begin{minipage}[b]{0.13\columnwidth}\centering\strut
chlorides\strut
\end{minipage} & \begin{minipage}[b]{0.24\columnwidth}\centering\strut
free\_sulfur\_dioxide\strut
\end{minipage} & \begin{minipage}[b]{0.25\columnwidth}\centering\strut
total\_sulfur\_dioxide\strut
\end{minipage} & \begin{minipage}[b]{0.10\columnwidth}\centering\strut
density\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.14\columnwidth}\centering\strut
\textbf{good}\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\centering\strut
0.076\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering\strut
14.88\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\centering\strut
37.28\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
0.996\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.14\columnwidth}\centering\strut
\textbf{medium}\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\centering\strut
0.09\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering\strut
16.6\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\centering\strut
48.43\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
0.997\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.14\columnwidth}\centering\strut
\textbf{poor}\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\centering\strut
0.072\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering\strut
8.333\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\centering\strut
26.94\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
0.997\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}cccc@{}}
\toprule
\begin{minipage}[b]{0.16\columnwidth}\centering\strut
~\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\centering\strut
pH\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\centering\strut
sulphates\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\centering\strut
alcohol\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.16\columnwidth}\centering\strut
\textbf{good}\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering\strut
3.307\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\centering\strut
0.749\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
11.48\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.16\columnwidth}\centering\strut
\textbf{medium}\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering\strut
3.304\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\centering\strut
0.655\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
10.25\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.16\columnwidth}\centering\strut
\textbf{poor}\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering\strut
3.379\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\centering\strut
0.539\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
10.31\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

We would also like to address the fact that we have excluded two data
points from the original data set. These were observations 1080 and
1082. These data points were excluded due to their being response
outliers. We know this because their residual to leverage ratio is very
large. If these points were included, the fitted models would be
drastically different. For example, the flexible models would likely do
better whereas the nonflexible models would do more poorly. Furthermore,
we would also like to point out that several variable are highly
correlated. For example fixed acidity, citric acid, and pH are all
highly correlated. This would make sense to many people with a chemical
background and to some mathematicians in hindsight. For instance pH is a
measure of acidity. This collinearity is likely why LASSO shrinks
several variables to zero and many more closely to zero. It is also
likely why the PCR, PLS and ridge regression models produced similar
processes while the full linear regressions suffers greatly.

As discussed, there existed some issues with the dataset. As we used a
publically available dataset without indentifying details (wine name,
winery name, etc) we could not further explore the cause of potential
outlying values. Had this information been available we could have been
more confident with the elimination of outliers and other discerning
information.

The predictors, while loosely normal were not normal as evidence of a
Shapiro Wilke test on all the predictors (all p values \textless{}
0.01). Log, square root and polynomial transforms were attempted to
normalize the data but these transforms did not improve the normality of
the predictors. Because of this fact we could have performed an advanced
transformation to these values like a Boxcox transform, but this would
have made interpretability of the resulting models much more difficult.
As such these transforms were not used for sake of this analysis.

\section{Conclusion}\label{conclusion}

The best method for prediction is the boosted regression model. The best
method for inference is the Lasso regression model. The best method for
classifying into high, medium, and low quality wine is the quadratic
discriminant analysis technique. Two data points were found to be
response outliers and were removed. The distributions of the predictor
variables do not seem to have a serious effect on the fitted models. The
highly correlated variables do have an effect on the fit of the models,
depending on the strengths and weaknesses of the various model fitting
processes.


\end{document}
