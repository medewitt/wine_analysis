---
title: "Final Report for Red Wine Analysis"
author: "Ruiqiang Chen, Michael DeWitt, David Williams, Alex Vannoy"
date: "7/28/2017"
header-includes:
   - \usepackage{amsmath}
   - \usepackage{float}

output:
  pdf_document:
    number_sections: yes
    keep_tex: yes
  html_document:
    fig_caption: yes
    fig_height: 4
    number_sections: yes
    pandoc_args:
    - +RTS
    - -K64m
    - -RTS
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.pos = "H")
library(ProjectTemplate)
library(ggfortify)
load.project()

(filenames <- list.files("src", pattern="*.R", full.names=TRUE))

for( a in 1:length(filenames)){
  on.exit(filenames[a])
  source(paste0(filenames[a]))
}
```

# Introduction

The purpose of this document is to report the proposed statistical models for classification of red wine bases on 11 predictors. The purpose of this analysis is to provide a model to the vintners in order for them to better predict the quality rating for their product. Analysis will be performing using both regression techniques and classification techniques.

# Description of Data
The data set provided is the Wine dataset from UC Irvine of red _vinho verde_ wine samples, from the north of Portugal [Cortez et al., 2009]. It consists of `r nrow(red_wine_data_raw) ` with a total of `r ncol(red_wine_data_raw) -1` physicochemical predictors and a response variable. These predictors include the following: `r names(red_wine_data_raw)` with the quality feature being associated with the judgement of the individual wine's quality. Quality is the feature of interest for the dataset as the vintner is interested in judging the wine's quality through objective means rather than today's subjective method of averaging the 1-10 point judgment of taste-testers. A summary of these measures as well as the response variable can be seen in Table 1. 

```{r summary_table, echo = FALSE}
knitr::kable(table_to_print, booktabs = TRUE,
             caption = "Summary Statistics for the Wine Dataset")
```


The distribution of these different criteria can be seen below in the histograms in Figure 1.

```{r global_histograms, echo=FALSE, warning=FALSE, message=FALSE, fig.cap= "Histogram of all variables in the data set", fig.asp= .5}
red_wine_data_index <-red_wine_data_raw%>% 
  mutate(id = seq.int(nrow(red_wine_data_raw)))
as.data.frame(red_wine_data_index) %>% 
  melt(id.vars = "id") %>% 
  ggplot(aes(value))+facet_wrap(~variable, scales = "free_x")+geom_histogram()+
  geom_histogram(binwidth = function(x) 2 * IQR(x) / (length(x)^(1/3)))
```

The following are slightly right skewed: Fixed Acidity, Volatile Acidity, Citric Acid, , Free Sulfur Dioxide, Total Sulfur Dioxide, Sulphates, and Alcohol. Residual Sugar and Chlorides are heavily right skewed with density and pH appearing more normally distributed. Completing a Shapiro Wilke normality test on the components indicates that all are non-normal. Reviewing the individual components there appears to be a slight irregularity with total free sulfur dioxide. This can be seen in the histogram of this variable. 

```{r so2_fig, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="Histogram of Sulfur Dioxide Predictor", fig.asp= 0.5}
as.data.frame(red_wine_data_factors) %>% 
  melt() %>%
  filter(variable == "free_sulfur_dioxide") %>% 
  ggplot(aes( value))+
  geom_histogram(bins = 30)+
  # labs(
  #   title = "Histogram of Free Sulfur Dioxide",
  #   caption = "From UCI Wine Data Set"
  # )+
  xlab("Free Sulfur Dioxide")
```

As well as the fit of free sulfur dioxide display high studentized residuals and leverage and thus should be considered for removal in the modeling process. These wines are 1080 and 1082.

```{r so2_resid, echo=FALSE, warnings = FALSE, fig.cap="Residual Plots of Linear Model Predicting Quality with Total Sulfur Dioxide ", fig.asp= .5}
so2_resid <-autoplot(fit2)
print(so2_resid)
```

These two wines have been removed from the clean dataset in order to be better predictors. The presence of these two wines may result in incorrect or inaccurate predictions. Ass we did not gather this dataset, we do not know if this information was incorrectly captured or if these values are real. However, given the strong indication that these two points are outliers with high leverage it is a good assumption to remove these two points.

# Method
In order to estimate the test error of any of the generated models, the data was divided in testing and training data sets with which to train then models and then test and estimate the testing error. Seventy percent of the raw data was randomly selected and placed in the training set with the remaining thirty percent used as the testing data set.

## Regression

In order to select the best fit regression model, several different modeling methods were produced. These include Least Squares Regression, Ridge Regression, Lasso Regression, Principle Components Regression, and Partial Least Squares Regression. The quality integer was the value that the model attempts to predict for each of these methods. The data was divided into two sets, a training set to train the model and a testing set for model validation. We will now go deeper in the model generation process for each of these different modeling types and methods.

### Least Squares
The least squares regression method that was tested was the best subset selection. The methodology used to determine the best subset model was to first run cross-validation on the training set in order to determine the best number of predictors to include in the model. This analysis indicated that any additional predictor after three variables were selected did not increase the accuracy of the model. The training data was then used to determine the best subset of the linear model with three predictors. The best subset included:

#### Residual Analysis
Here we need to make some plots against of the fit vs predictors and fit vs prediction to cross off that we considered our residuals

```{r residuals, echo=FALSE, warning=FALSE, message=FALSE, fig.cap= "Plot of Residuals from Linear Regression", fig.asp= .5}
print(linear_resid)
```

The residuals appear to have no distinct pattern which is a positive sign that there are not lurking relationships that have not been treated by the modeling.


### Ridge Regression
Ridge regression was performed on the dataset as well. Cross-validation was performed on the training data set to determine the optimum value for lambda for the ridge regression. This lambda, `r round(bestlam_ridge,3)` was then used in a ridge regression model with the testing dataset. Further, there seems to be a very large coefficient with 11 much smaller coefficients.

### Lasso Regression
Lasso regression was used with cross-validation on the data set. Cross-validation was used to determine the best lambda which was `r round(bestlam_lasso,3)`. As a function of the lasso regression only pH was shrunk to zero with total sulphur dioxide and free sulphur dioxide being shrunk to near zero. Also similar to ridge regression, there appears to be one much larger coefficient in relation to the others.

```{r lasso, echo=FALSE, warning=FALSE, message=FALSE, out.width=c('225px', '225px'), fig.align = "center", fig.show = "hold", fig.cap= "Plot of Lambda vs Coefficients for Ridge/Lasso Regression"}
plot(ridge_mod, xvar = "lambda", label = TRUE, main = "Ridge Regression")
plot(lasso_mod, xvar = "lambda", label = TRUE, main = "Lasso Regression")
```


### Principal Components Regression
Principal components regression was used. Based on the analysis of the principal components, the first nine principal components were used to be trained on the training set. This was done because 90% of the variation could be explained by these first nine components. This is graphically displayed in the plot of principal components.

```{r pcr_fig, echo=FALSE, warning=FALSE, message=FALSE, fig.cap= "Variance Explained for Principal Components Regression", fig.asp= .5}
pcr_fit <- pcr( quality ~ ., data=red_wine_data_training, scale=TRUE, validation="CV" )
pcr_x<-pcr_fit$Xvar/sum(pcr_fit$Xvar)
plot(cumsum(pcr_x), xlab = "Number of Components", ylab = "Cumulative Porportion of Variance Explained", ylim = c(0,1))
```


### Partial Least Squares Regression
Partial least squares regression was used. However, the difference is that it uses quality response as supervision over the principal components. Using this method one can see from the plot of patial least squares components that after roughly 2-3 components, the model accuracy does not increase drastically.

```{r pls_fig, echo=FALSE, warning=FALSE, message=FALSE, fig.cap= "Variance Explained for Partial Least Squares Regression", fig.asp= .5}
pls_fit <- plsr( quality ~ ., data = red_wine_data_clean, subset = train_rows, 
                 scale=TRUE, validation="CV" )
pls_x<-pls_fit$Xvar/sum(pls_fit$Xvar)
plot(cumsum(pls_x), xlab = "Number of Components", ylab = "Cumulative Porportion of Variance Explainned", ylim = c(0,1))
```


### Boosted Regression
Boosted regression was also used. The interaction depth was limited to four in order to reduce the likelihood of over-fitting the data. The model was trained on 5,000 different trees. 

```{r boost_importance, warning= FALSE, echo = FALSE, fig.asp= .5, fig.cap="Relative Importance from Boosted Regression", fig.pos="H"}
print(boost_reg_plot)
```


### Model Selection
The resulting mean squared errors for each regression method were tabulated in order to determine the superior model.

```{r mse_results, warning= FALSE, echo = FALSE, fig.asp= .5, fig.cap="Plot of Results of Different Regression Techniques", fig.pos="H"}
print(MSE_plot)
```

## Classification
For classification purposes the wines were segregated in to three different classes. These classes include "good" ($quality >7$), "medium" ($\; quality \; between \; 4 \;and\;  7$) and "poor"($quality < 4$). 

### Model Selection
Several different classification models were used in this analysis given the new variable added to the data set. The methods used were K-Nearest Neighbors, Linear Discriminant Analysis, Quadratic Discriminant Analysis, and tree classification. These different models were trained on the training data set and then applied to the testing dataset to estimate the accuracy. It is important to note that greater accuracy was achieved by scaling values for the K-Nearest Neighbors approach as this approach uses Euclidean distances and thus is sensitive to scale differences. The phenomena can be seen as with the unscaled values the validation algorithm found that `r unscaled_neighbours` were used versus `r scaled_knn`. The larger number of neighbours makes for a much more global model, less sensitive to immediate neighbours in the bias versus variance trade off. The tree classification model was trained first through cross-validation and then pruned to six leaves in order to reduce the impact of over-fitting in the bias variance trade off.

## Comparison of Models
```{r classification_results, echo=FALSE, message=FALSE, warning=FALSE, fig.asp= .5, fig.cap="Plot of Results of Different Classification Techniques", fig.pos= "H"}
print(classification_plot)
```

All of these models seek to maximize the global accuracy of the model. More interesting for the vintners is the ability to detect each of the three different classes of the wines.

```{r classification_details, echo=FALSE, message=FALSE, warning=FALSE, fig.asp= .5}

knitr::kable(combined_detailed_classification_by_category, booktabs = TRUE,
             caption = "Detailed Classification Accuracy")

```


# Discussion

stuff to write about
\pagebreak

# Conclusion
This analysis shows that for regression the boosted regression resulted in the highest accuracy of all regression models; however, this accuracy comes at a cost of interpretability. Because the boosted algorithms have little intrepreation this accuracy is more for prediction than inference. If inference is the goal for the vintner and horticulturalists who seek to understand the properties that make good wines, the model with higher interpretability and the second highest accuracy is the Lasso regression model. While the PLS is more accurate, again it suffers from ease of interpretation. Thus with this in mind, the superior model for inference with high accuracy is characterized by the below equation:\linebreak

\begin{equation}
\begin{aligned}
\label{lasso_eq}
quality = 39.37 + 0.0823 * fixed\;acidity -0.981 * volatile\;acidity -0.405 * citric\;acid \\
-0.013 *residual\;sugar -1.075 * chlorides + 0.006 * free\;sulfur\;dioxide\\
- 0.002 * total\;sulfur\;dioxide - 37.09 * density +1.032 * sulphates + 0.256 * alcohol
\end{aligned}
\end{equation}

Thus from equation \ref{lasso_eq} the vintner can examine each of the variables independently and provide some degree of inference regarding the chemical levels that influence the quality of the red wine. For instances one can see that sulfate content appears to have a stronger positive influence on the wine quality while wines with additional residual sugars reduce the quality score. In the hands of the vinter, these relationships can be explored or potentially exploited to produce a higher quality wine more consistently. \linebreak

Turning to the classification method, the best overall classification method was Quadratic Discriminate Analysis. This is seen in both the overall accuracy as well in its ability to accurately classify each subcategory. While the other methods have lesser abilities to detect the good and poor quality wines, the QDA method showed the best accuracy in these two fields, which is very important for vintners when it comes to pricing and selling. The penalty of misclassifying a good wine as medium or a poor wine as medium/ good is severe as this may damage the reputation of the winery. From this analysis it is clear that QDA is the superior method for classification of red wines given this dataset.

# Discussion of Issues

Outlier filtering
Colinnearity (some points became clear were related fixed acidity, citric acid, pH. Some methods did better filtering this impact PCR, PLS, Lasso, Ridge, while linear regression suffered a little, but we removed pH as a predictor and that helped.)
Non-normality -> we could have done some advanced transformations like boxcox, but this would improve model accuracy at the expense of inference.
